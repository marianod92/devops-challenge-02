name: 'terraform-plan-apply-destroy-with-input'

# on: 
#   workflow_dispatch:
#     inputs:
#       logLevel:
#         description: 'Log level'     
#         required: true
#         default: 'info'
#       tags:
#         description: 'Set terraform action: Apply / Destroy' 

on:
  workflow_dispatch:
    inputs:
      action:
        type: choice
        description: Select Terraform Action
        options: 
        - apply
        - plan
        - destroy
        - get-nakame-url
      stack:
        type: choice
        description: Select Terraform Stack
        options: 
        - ecr
        - eks
        - vpc

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

env:
  name: "nakama"
  AWS_ACCESS_KEY_ID: ${{ secrets.aws_access_key }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.aws_secret_access_key }}
  AWS_REGION: ${{ secrets.aws_region }}
  base_working_directory: "devops/terraform/aws/nakama"

jobs:
  terraform:
    name: 'Terraform'
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1

    - name: Check Terraform Version
      id: version
      run: terraform --version

    - name: Terraform Init
      id: init
      run: terraform init
      working-directory: "${{ env.base_working_directory }}/${{  github.event.inputs.stack }}"

    - name: Terraform Format
      id: fmt
      run: terraform fmt -check
      working-directory: "${{ env.base_working_directory }}/${{  github.event.inputs.stack }}"

    - name: Terraform Plan
      id: plan
      if: github.ref == 'refs/heads/master' && github.event.inputs.action == 'plan'
      run: terraform plan
      working-directory: "${{ env.base_working_directory }}/${{  github.event.inputs.stack }}"

    - name: Terraform Apply | Destroy
      if: github.ref == 'refs/heads/master' && github.event.inputs.action == 'apply'
      run: echo "${{github.event.inputs.action}}"; terraform ${{github.event.inputs.action}} -auto-approve
      working-directory: "${{ env.base_working_directory }}/${{  github.event.inputs.stack }}"

    - name: Update kubeconfig
      if: github.event.inputs.action == 'apply' || github.event.inputs.action == 'get-nakame-url'
      run: aws eks update-kubeconfig --name $(aws eks list-clusters | jq -r '.clusters[0]') ; export KUBECONFIG=$(cat $HOME/.kube/config | base64)

    # - name: Login to Nakama Console
    #   if: github.event.inputs.action == 'apply' || github.event.inputs.action == 'get-nakame-url'
    #   uses: actions-hub/kubectl@master
    #   env:
    #     KUBE_CONFIG: ${{env.KUBECONFIG}}
    #   with:
    #    args: get svc ${{ env.name }} -n ${{ env.name }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'

    - name: Install and configure kubectl
      run: |
        VERSION=$(curl --silent https://storage.googleapis.com/kubernetes-release/release/stable.txt)
        curl https://storage.googleapis.com/kubernetes-release/release/$VERSION/bin/linux/amd64/kubectl \
            --progress-bar \
            --location \
            --remote-name
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        echo ${{ secrets.KUBECONFIG }} | base64 --decode > kubeconfig.yaml

    - name: Retrieve Nakama Console URL      
    - id: retrieve-url
      run: |
           nakama_dns=$(kubectl get svc ${{ env.name }} -n ${{ env.name }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
           echo "::add-mask::$nakama_dns"
           echo "::set-output name=namaka-dns::$nakama_dns"

    - name: Login into Nakama Console 
      run: |
          echo "the secret number is ${{ steps.retrieve-url.outputs.nakama_dns }}"

    - name: Prune
      run: rm -rf $HOME/.kube/config; unset AWS_SECRET_KEY; unset AWS_SECRET_ACCESS_KEY

#aws elb describe-load-balancers --output text --query 'LoadBalancerDescriptions[].DNSName'
